import org.apache.log4j.Level;

import org.apache.log4j.Logger;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.functions;
import org.apache.spark.sql.expressions.Window;
import org.apache.spark.sql.expressions.WindowSpec;
import org.apache.spark.ml.feature.Bucketizer;
import org.apache.spark.ml.feature.Imputer;
import org.apache.spark.ml.feature.VectorAssembler;
import org.apache.spark.rdd.RDD;
import org.apache.spark.ml.feature.ImputerModel;
import org.apache.spark.ml.feature.MaxAbsScaler;
import org.apache.spark.ml.feature.MaxAbsScalerModel;
import org.apache.spark.ml.feature.Normalizer;
import org.apache.spark.ml.feature.StringIndexer;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.RelationalGroupedDataset;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SQLContext;

import static org.apache.spark.sql.functions.col;
import org.apache.spark.ml.recommendation.ALS;
import org.apache.spark.ml.recommendation.ALSModel;
import org.apache.spark.ml.evaluation.*;
import com.github.fommil.netlib.*;
import org.apache.spark.sql.DataFrameNaFunctions;
import org.apache.spark.ml.clustering.KMeansModel;
import org.apache.spark.ml.clustering.BisectingKMeans;
import org.apache.spark.ml.clustering.BisectingKMeansModel;
import org.apache.spark.ml.clustering.KMeans;
import org.apache.spark.ml.evaluation.ClusteringEvaluator;
import org.apache.spark.ml.linalg.Vector;
import org.apache.spark.sql.DataFrameReader;
import org.apache.spark.sql.DataFrameWriter;

import com.amazonaws.auth.AWSStaticCredentialsProvider;
import com.amazonaws.auth.BasicAWSCredentials;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.GetObjectRequest;
import com.amazonaws.services.s3.model.S3Object;
import org.apache.hadoop.fs.s3a.*;

import java.util.Arrays;
import java.util.List;

import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;

import scala.Tuple2;
import scala.collection.Seq;

import org.apache.hadoop.classification.InterfaceStability.Stable;
import static org.apache.spark.sql.functions.*;

public class AnalyticsClustering {
	
	public static void main (String[] args) {
		
		System.out.println("This is somthing idiotic");
		
		Logger.getLogger("org").setLevel(Level.ERROR);
				SparkSession sparkSession = SparkSession.builder().appName("AnalyticsClustering").master("local[*]").getOrCreate();
		
		
		
		Dataset<Row> ClickStreamData = sparkSession.read().option("header", false).option("inferschema", false)
			.csv("/Users/ravig/Documents/Upgrad/Complete/Course6/Project/sample100mb.csv")
			//.csv("/Users/ravig/Documents/Upgrad/Complete/Course6/Project/sample1000.csv")
			.toDF("UserID","epoch_seconds","SongID","Date");
		
		Dataset<Row> SongsMetaData = sparkSession.read().option("header", false).option("inferschema", false)
				.csv("/Users/ravig/Documents/Upgrad/Complete/Course6/Project/newmetadata/part-m-*")
				.toDF("SongID","ArtistID");
		
		Dataset<Row> SongsMetaData_nonull = SongsMetaData.na().drop("any");
		SongsMetaData_nonull.show();
		
		Dataset <Row> Notification_Clicks = sparkSession.read().option("header", false).option("inferschema", false)
				.csv("/Users/ravig/Documents/Upgrad/Complete/Course6/Project/notification_clicks/Consolidated_clicks")
				.toDF("NotificationID","UserID","Date");
		Dataset <Row> Notification_Clicks_nodate = Notification_Clicks.drop("Date");
		Dataset <Row> Notification_Clicks_clean = Notification_Clicks_nodate.distinct();
		
		Dataset <Row> Notification_details = sparkSession.read().option("header", false).option("inferschema", false)
				.csv("/Users/ravig/Documents/Upgrad/Complete/Course6/Project/notification.csv")
				.toDF("NotificationID","TopArtist");
		
		
		Dataset<Row> ClickStreamData_new = ClickStreamData.select(col("UserID"),col("epoch_seconds").cast("Integer"),col("SongID"),col("Date"));
		Dataset<Row> ClickStreamData_count = ClickStreamData_new.withColumn("Count", functions.lit(1));
		
		ClickStreamData_count.show();
		ClickStreamData_count.printSchema();
		
		
		Dataset<Row> UniqueSongHitPerDay = ClickStreamData_count.groupBy("UserID","SongID","Date").sum("Count")
				.withColumnRenamed("sum(Count)", "ListenedCount");
		
		System.out.println("Printing ClickStream Data with UniqueSongHitPerDay");
		
		
		
		//UniqueSongHitPerDay.orderBy("ListenedCount").show();
		//UniqueSongHitPerDay.orderBy(org.apache.spark.sql.functions.col("ListenedCount").desc()).show();
		//UniqueSongHitPerDay.printSchema();
		//UniqueSongHitPerDay.show();
		
		System.out.println("Printing ClickStream Data with TotalSongsPerDay");
		
		Dataset<Row> User_TotalSongsPerDay = UniqueSongHitPerDay
				.groupBy("UserID","Date").sum("ListenedCount")
				.withColumnRenamed("sum(ListenedCount)", "TotalSongsPerDay");
				
				//User_TotalSongsPerDay.show();
		
		
		System.out.println("Printing ClickStream Data with Unique Songs hit and TotalSongsPerDay");
		
		
	
		Dataset<Row> User_Unique_songs_Total_songs_count = UniqueSongHitPerDay.join(User_TotalSongsPerDay, 
				User_TotalSongsPerDay.col("UserID").equalTo(UniqueSongHitPerDay.col("UserID"))
				.and(User_TotalSongsPerDay.col("Date").equalTo(UniqueSongHitPerDay.col("Date"))))
				.drop(User_TotalSongsPerDay.col("Date")).drop(UniqueSongHitPerDay.col("UserID"));
				
		
		//User_Unique_songs_Total_songs_count.show();
		
		System.out.println("Printing TotalSongsPerUser Songs ");
		
		Dataset<Row> TotalSongsPerUser = ClickStreamData_count.groupBy("UserID").sum("Count")
				.withColumnRenamed("sum(Count)","TotalSongsPlayedoverall");
		//TotalSongsPerUser.orderBy("TotalSongsPlayedoverall").show();
		//TotalSongsPerUser.orderBy(org.apache.spark.sql.functions.col("TotalSongsPlayedoverall").desc()).show();
		//TotalSongsPerUser.printSchema();
		
		
		Dataset<Row> User_Unique_songs_Total_songs_count_overall = User_Unique_songs_Total_songs_count.join(TotalSongsPerUser,"UserID");
		
		User_Unique_songs_Total_songs_count_overall.orderBy(org.apache.spark.sql.functions.col("TotalSongsPlayedoverall").desc()).show();
		
		
		System.out.println("User_Unique_songs_Total_songs_count with Count with ArtistID");
		
		Dataset<Row> User_Unique_songs_Total_songs_count_artist = User_Unique_songs_Total_songs_count_overall.join(SongsMetaData_nonull, "SongID");
		User_Unique_songs_Total_songs_count_artist.show();
		//User_Unique_songs_Total_songs_count_artist.printSchema();
		Dataset<Row> User_Unique_songs_Total_songs_count_artist_clean = User_Unique_songs_Total_songs_count_artist.na().drop("any");
		Dataset<Row> User_Unique_songs_Total_songs_gt_ten = User_Unique_songs_Total_songs_count_artist_clean;
														//.filter(User_Unique_songs_Total_songs_count_artist_clean.col("TotalSongsPlayedoverall").gt("10"));
		
		//User_Unique_songs_Total_songs_count_artist_clean.show(100);
		User_Unique_songs_Total_songs_gt_ten.show();

		VectorAssembler assembler = new VectorAssembler()
				  .setInputCols(new String[] {"ListenedCount", "TotalSongsPerDay", "TotalSongsPlayedoverall"}).setOutputCol("features");
	
		Dataset<Row> datasetRfm = assembler.transform(User_Unique_songs_Total_songs_gt_ten);
		datasetRfm.show();
		Dataset<Row> datasetRfm_drop = datasetRfm.drop(new String[] {"SongID","Date","ListenedCount","TotalSongsPerDay","TotalSongsPlayedoverall"});
		datasetRfm_drop.show();
		
		KMeans kmeans = new KMeans().setK(4);
		KMeansModel model = kmeans.fit(datasetRfm);
		
		//Trains a Bisecting K means Model
		//BisectingKMeans BKM = new BisectingKMeans();
		//BisectingKMeansModel model = BKM.fit(datasetRfm);
		//BisectingKMeansModel model = BKM.fit(datasetRfm_drop);
		
		Dataset<Row> predictions = model.transform(datasetRfm_drop);
		predictions.show(200);
		Dataset<Row> predictions_clean = predictions.drop("features");
		
		Dataset<Row> predictions_count = predictions_clean.groupBy("prediction","ArtistID").count();
		predictions_count.show(100);
		Dataset<Row> Rank = predictions_count.withColumn("rank", dense_rank().over(Window.partitionBy("prediction")
				.orderBy(desc("count"))));
		Rank.show();
		Rank.printSchema();
		
		Dataset<Row> Top_Rank = Rank.filter(Rank.col("rank").equalTo(1));
		
				//select("prediction","ArtistID","count").where(Rank.col("rank").equalTo("1"));
		Top_Rank.show();
		Dataset<Row> Top_Rank_Rename = Top_Rank.withColumnRenamed("ArtistID", "TopArtist");
		
		Dataset<Row> cluster_top = predictions.join(Top_Rank_Rename,"prediction").drop("count").drop("rank").drop("features");
		
		
		System.out.println("Cluster details after dropping unwanted columns");
		cluster_top.show();
		
 		
		/* Calculating CTR for Notification ID 9553 */
		
		
		/*Filtering Out the list of Users who have clicked on Notification received for the ID 9659 */
		
		Dataset <Row> Notification_user_9553 = Notification_Clicks_clean.filter(Notification_Clicks_clean.col("NotificationID").equalTo(9553));
		Dataset <Row> Artists_notify_9553 = Notification_details.filter(Notification_details.col("NotificationID").equalTo(9659));
		
		System.out.println("List of Users who have clicked for Notification 9659");
		Notification_user_9553.show();
		
		System.out.println("List of Artists for Notification 9659");
		Artists_notify_9553.show();
		
		Dataset <Row> cluster_users_notified = cluster_top.join(Artists_notify_9553,"TopArtist");
		System.out.println("List of Users to whom we  sent Notification 9659");
		cluster_users_notified.show();
		
		Dataset<Row> cluster_clicks = cluster_users_notified.join(Notification_user_9553,"UserID");
		System.out.println("List of Users to who clicked on Notification 9659");
		cluster_clicks.show();
		
		long Clicks = cluster_clicks.count();
		long Count = cluster_users_notified.count();
		double CTR = 0.0;
		if (Clicks != 0)
		{
			CTR = Count/Clicks;	
		}
		else
		{
			System.out.println("CTR cannot be calculated since notificaiton was not sent to any users");
		}
		
		
		System.out.println("Click Stream ratio for 9553 is " +CTR);
		
		
/*Filtering Out the list of Users who have clicked on Notification received for the ID 9660 */
		
		Dataset <Row> Notification_user_9660 = Notification_Clicks_clean.filter(Notification_Clicks_clean.col("NotificationID").equalTo(9660));
		Dataset <Row> Artists_notify_9660 = Notification_details.filter(Notification_details.col("NotificationID").equalTo(9660));
		
		System.out.println("List of Users who have clicked for Notification 9660");
		Notification_user_9660.show();
		
		System.out.println("List of Artists for Notification 9660");
		Artists_notify_9660.show();
		
		Dataset <Row> cluster_users_9660_notified = cluster_top.join(Artists_notify_9660,"TopArtist");
		System.out.println("List of Users to whom we  sent Notification 9660");
		cluster_users_notified.show();
		
		Dataset<Row> cluster_9660_clicks = cluster_users_notified.join(Notification_user_9660,"UserID");
		System.out.println("List of Users to who clicked on Notification 9660");
		cluster_clicks.show();
		
		Clicks = cluster_9660_clicks.count();
		Count = cluster_users_9660_notified.count();
		CTR = 0.0;
		if (Clicks != 0)
		{
			CTR = Count/Clicks;	
		}
		else
		{
			System.out.println("CTR cannot be calculated since notificaiton was not sent to any users");
		}
		
		
		System.out.println("Click Stream ratio for 9660 is " +CTR);

		
/*Filtering Out the list of Users who have clicked on Notification received for the ID 9690 */
		
		Dataset <Row> Notification_user_9690 = Notification_Clicks_clean.filter(Notification_Clicks_clean.col("NotificationID").equalTo(9690));
		Dataset <Row> Artists_notify_9690 = Notification_details.filter(Notification_details.col("NotificationID").equalTo(9690));
		
		System.out.println("List of Users who have clicked for Notification 9690");
		Notification_user_9690.show();
		
		System.out.println("List of Artists for Notification 9690");
		Artists_notify_9690.show();
		
		Dataset <Row> cluster_users_9690_notified = cluster_top.join(Artists_notify_9690,"TopArtist");
		System.out.println("List of Users to whom we  sent Notification 9690");
		cluster_users_notified.show();
		
		Dataset<Row> cluster_9690_clicks = cluster_users_notified.join(Notification_user_9690,"UserID");
		System.out.println("List of Users to who clicked on Notification 9690");
		cluster_clicks.show();
		
		Clicks = cluster_9690_clicks.count();
		Count = cluster_users_9690_notified.count();
		CTR = 0.0;
		if (Clicks != 0)
		{
			CTR = Count/Clicks;	
		}
		else
		{
			System.out.println("CTR cannot be calculated since notificaiton was not sent to any users");
		}
		
		
		System.out.println("Click Stream ratio for 9690 is " +CTR);
		
/*Filtering Out the list of Users who have clicked on Notification received for the ID 9703 */
		
		Dataset <Row> Notification_user_9703 = Notification_Clicks_clean.filter(Notification_Clicks_clean.col("NotificationID").equalTo(9703));
		Dataset <Row> Artists_notify_9703 = Notification_details.filter(Notification_details.col("NotificationID").equalTo(9703));
		
		System.out.println("List of Users who have clicked for Notification 9703");
		Notification_user_9703.show();
		
		System.out.println("List of Artists for Notification 9703");
		Artists_notify_9703.show();
		
		Dataset <Row> cluster_users_9703_notified = cluster_top.join(Artists_notify_9703,"TopArtist");
		System.out.println("List of Users to whom we  sent Notification 9703");
		cluster_users_notified.show();
		
		Dataset<Row> cluster_9703_clicks = cluster_users_notified.join(Notification_user_9703,"UserID");
		System.out.println("List of Users to who clicked on Notification 9703");
		cluster_clicks.show();
		
		Clicks = cluster_9703_clicks.count();
		Count = cluster_users_9703_notified.count();
		CTR = 0.0;
		if (Clicks != 0)
		{
			CTR = Count/Clicks;	
		}
		else
		{
			System.out.println("CTR cannot be calculated since notificaiton was not sent to any users");
		}
		
		
		System.out.println("Click Stream ratio for 9703 is " +CTR);
		
		
/*Filtering Out the list of Users who have clicked on Notification received for the ID 9551 */
		
		Dataset <Row> Notification_user_9551 = Notification_Clicks_clean.filter(Notification_Clicks_clean.col("NotificationID").equalTo(9551));
		Dataset <Row> Artists_notify_9551 = Notification_details.filter(Notification_details.col("NotificationID").equalTo(9551));
		
		System.out.println("List of Users who have clicked for Notification 9551");
		Notification_user_9551.show();
		
		System.out.println("List of Artists for Notification 9551");
		Artists_notify_9551.show();
		
		Dataset <Row> cluster_users_9551_notified = cluster_top.join(Artists_notify_9551,"TopArtist");
		System.out.println("List of Users to whom we  sent Notification 9551");
		cluster_users_notified.show();
		
		Dataset<Row> cluster_9551_clicks = cluster_users_notified.join(Notification_user_9551,"UserID");
		System.out.println("List of Users to who clicked on Notification 9551");
		cluster_clicks.show();
		
		Clicks = cluster_9551_clicks.count();
		Count = cluster_users_9551_notified.count();
		CTR = 0.0;
		if (Clicks != 0)
		{
			CTR = Count/Clicks;	
		}
		else
		{
			System.out.println("CTR cannot be calculated since notificaiton was not sent to any users");
		}
		
		
		System.out.println("Click Stream ratio for 9551 is " +CTR);
		
		
		
		
		
		
		// Evaluate clustering by computing Silhouette score
				//ClusteringEvaluator evaluator = new ClusteringEvaluator();

				//double silhouette = evaluator.evaluate(predictions);
				//System.out.println("Silhouette with squared euclidean distance = " + silhouette);

				// Shows the result
				//Vector[] centers = model.clusterCenters();
				//System.out.println("Cluster Centers: ");
				//for (Vector center : centers) {	
				//	System.out.println(center);
				//}
				
		
		//Dataset<Row> SongsFrequencyOverall = ClickStreamMetaData_clean.groupBy("UserID","Date").sum("ListenedCount")
			//	.withColumnRenamed("sum(ListenedCount)", "TotalSongsPerDay");
		
		//SongsFrequencyOverall.orderBy("TotalSongsPerDay").show();
		
		
		
		
		//ClickStreamMetaData_nonull.orderBy("ListenedCount").show(300);
		//ClickStreamMetaData_nonull.orderBy(org.apache.spark.sql.functions.col("ListenedCount").desc()).show();
		//ClickStreamMetaData_nonull.printSchema();
		
		
		
		
		//Dataset<Row> SongsFrequencyOverall = ClickStreamData_count.groupBy("UserID","SongID").sum("Count")
		//	.withColumnRenamed("sum(Count)", "SongsHitOverall");
		
		

		
		
		//SongsFrequencyOverall.orderBy("SongsHitOverall").show();
		//SongsFrequencyOverall.orderBy(org.apache.spark.sql.functions.col("SongsHitOverall").desc()).show();
		//SongsFrequencyOverall.printSchema();
		
		//Dataset<Row> ClickStreamMetaData_clean_overall = 
		//		ClickStreamMetaData_clean.join(SongsFrequencyOverall,
		//				(ClickStreamMetaData_clean.col("UserID").equalTo(SongsFrequencyOverall.col("UserID"))),"left");
						
		//Dataset<Row> ClickStreamMetaData_clean_overall_both = ClickStreamMetaData_clean.join(SongsFrequencyOverall)
		//		.where (ClickStreamMetaData_clean.col("UserID").equalTo(SongsFrequencyOverall.col("UserID"))
		//				.and (ClickStreamMetaData_clean.col("SongID").equalTo(SongsFrequencyOverall.col("SongID"))));
		
		//ClickStreamMetaData_clean_overall_both.show();
		
			
				
		/*

		Dataset<Row> SongsPerDate = ClickStreamData_count.groupBy("UserID","Date").sum("Count")
				.withColumnRenamed("sum(Count)","TotalPlayedperDay");
		SongsPerDate.orderBy("TotalPlayedperDay").show();
		SongsPerDate.orderBy(org.apache.spark.sql.functions.col("TotalPlayedperDay").desc()).show();
		SongsPerDate.printSchema();
		
				
		
		System.out.println("Uniques Users in ListenedCount per User Table--> " +SumValue.count());
		System.out.println("Uniques Users in TotalPlayed per User Table--> " +SongsPerDate.count());
		*/
	}

}
