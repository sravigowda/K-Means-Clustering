import org.apache.log4j.Level;

import org.apache.log4j.Logger;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.functions;
import org.apache.spark.sql.expressions.Window;
import org.apache.spark.sql.expressions.WindowSpec;
import org.apache.spark.ml.feature.Bucketizer;
import org.apache.spark.ml.feature.Imputer;
import org.apache.spark.ml.feature.VectorAssembler;
import org.apache.spark.rdd.RDD;
import org.apache.spark.ml.feature.ImputerModel;
import org.apache.spark.ml.feature.MaxAbsScaler;
import org.apache.spark.ml.feature.MaxAbsScalerModel;
import org.apache.spark.ml.feature.Normalizer;
import org.apache.spark.ml.feature.StringIndexer;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.RelationalGroupedDataset;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SaveMode;
import org.apache.spark.ml.recommendation.ALS;
import org.apache.spark.ml.recommendation.ALSModel;
import org.apache.spark.ml.clustering.KMeans;
import org.apache.spark.ml.clustering.KMeansModel;
import org.apache.spark.ml.evaluation.*;
import com.github.fommil.netlib.*;
import org.apache.spark.sql.DataFrameNaFunctions;

import java.util.Arrays;
import java.util.List;

import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;

import scala.Tuple2;
import scala.collection.Seq;

import org.apache.hadoop.classification.InterfaceStability.Stable;
import static org.apache.spark.sql.functions.*;


public class Analytics {

	public static void main (String [] args) {
		
		
		Logger.getRootLogger().setLevel(Level.ERROR);
		Logger.getLogger("org").setLevel(Level.ERROR);
		
		SparkSession sparkSession = SparkSession.builder().appName("AnalyticsML").master("local[*]").getOrCreate();
		
		// We are loading user activity ClickStreamData into a DataSet
		
		Dataset<Row> ClickStreamData = sparkSession.read().option("header", false).option("inferschema", false)
				.csv("/Users/ravig/Documents/Upgrad/Complete/Course6/Project/sample100mb.csv")
				.toDF("UserID","epoch_seconds","SongID","Date");
		
		Dataset<Row> SongsMetaData = sparkSession.read().option("header", false).option("inferschema", false)
				.csv("/Users/ravig/Documents/Upgrad/Complete/Course6/Project/newmetadata/part-m-*")
				//.csv("/home/ec2-user/Analytics//newmetadata/part-m-*")
				.toDF("SongID","ArtistID");
		SongsMetaData = SongsMetaData.na().drop();
		
		Dataset <Row> Notification_Clicks = sparkSession.read().option("header", false).option("inferschema", false)
				.csv("/Users/ravig/Documents/Upgrad/Complete/Course6/Project/clicks/*")
				//.csv("/home/ec2-user/Analytics/Notification_clicks/Consolidated_clicks")
				.toDF("NotificationID","UserID","Date");
		
		/* 
		 * After loading the Notification data,we are dropping Date column since we are not going to use it. Along with this we are removing duplicates if any
		 * 
		 */
		
		Notification_Clicks = Notification_Clicks.drop("Date").distinct();
		
		Dataset <Row> Notification_details = sparkSession.read().option("header", false).option("inferschema", false)
				.csv("/Users/ravig/Documents/Upgrad/Complete/Course6/Project/notification.csv")
				//.csv("/home/ec2-user/Analytics/notification.csv")
				.toDF("NotificationID","ArtistID");
		
		
		/*
		 * We are casting timestamp into integer. We are dropping of this column since It was not needed. Below link talks about the same
		 * https://learn.upgrad.com/v/course/78/question/93683
		 */
		
		ClickStreamData = ClickStreamData.withColumn("Count", functions.lit(1)).drop("epoch_seconds");
		
		
			
		// We are calculating the number of times a user has listened to a particular song
		
		Dataset<Row> SumValue = ClickStreamData.groupBy("UserID","SongID").sum("Count")
				.withColumnRenamed("sum(Count)", "ListenedCount");
		
		
		
		// We are converting UserID and SongID to Double since ALS doesn't accept string variables
		
		WindowSpec w = Window.orderBy("UserID");
	
		
		Dataset<Row> userID_change = ClickStreamData.select("UserID").distinct().
				//withColumn("NewUserID",monotonically_increasing_id()); --> We are not using this since it creates problems with ALS
				withColumn("NewUserID",row_number().over(w));
		
		w = Window.orderBy("SongID");
		
		Dataset<Row> songID_change = ClickStreamData.select("SongID").distinct()
				//.withColumn("NewSongID", monotonically_increasing_id());
				.withColumn("NewSongID", row_number().over(w));
	
				
		// We are casting ListenedCount to Double
	
		Dataset<Row> SumValue_int = SumValue.select(col("UserID"),col("SongID"),col("ListenedCount").cast("Double"));
		
		SumValue_int.printSchema();
				
		
		
				
		/* In the following few lines of code, we are joining changed UserID and SongID to original dataset. After this we 
		 * dropping unwanted columns "UserID" and "SongID"
		 */
		
		Dataset<Row> ClickStreamData_Change = SumValue_int.join(userID_change,"UserID").join(songID_change,"songID");
		
		Dataset<Row> ClickStreamData_clean = ClickStreamData_Change.drop("UserID").drop("SongID");
		
		System.out.println("Just before ALS");
		
		ALS als = new ALS();
		long seed = 1800009193L;
		als.setMaxIter(20)
		   .setSeed(seed)
		   .setItemCol("NewSongID")
		   .setRatingCol("ListenedCount")
		   .setUserCol("NewUserID")
		   .setImplicitPrefs(true)
		   .setRank(12)
		   .setRegParam(0.2);
		   
		/*
		 * We are applying ALS on the ClickStream data and extract features from it. 
		 */
		
		Dataset<Row> userfeatures = als.fit(ClickStreamData_clean).userFactors();
		
		/* Now we are converting userFactors array into individual values to be applied in Vector Assembler */
		for(int i = 0; i < 12; i++) {
			userfeatures = userfeatures.withColumn("factor"+i, userfeatures.col("features").getItem(i));
		}
		
		userfeatures = userfeatures.drop("features");
		
		System.out.println("We have completed extracting features from ALS");
		
		/* Now we are building k-means model using the features derived from ALS
		 * We are planning to come up with 350 clusters
		 */
		VectorAssembler assembler = new VectorAssembler()
				  .setInputCols(new String[] {"factor0", "factor1", "factor2", "factor3", "factor4", "factor5", "factor6", "factor7"
						  , "factor8", "factor9", "factor10","factor11" })
				  .setOutputCol("features");
		
		Dataset<Row> KMeansInput = assembler.transform(userfeatures);
		KMeans kmeans = new KMeans().setK(350);
		KMeansModel Model = kmeans.fit(KMeansInput);
		
		
		
		//Make predictions and select cluster number and userID
		
		Dataset<Row> predictions = Model.transform(KMeansInput).select("id", "prediction");
		
		System.out.println("We have completed K-Stream");
		
		//predictions.show(100);
		predictions = predictions.withColumnRenamed("id", "NewUserID");
		Dataset<Row> clickstream_prediction = ClickStreamData_clean.join(predictions,"NewUserID");
		
		clickstream_prediction= clickstream_prediction.join(userID_change,"NewUserID").join(songID_change,"NewSongID");
		clickstream_prediction=clickstream_prediction.drop("NewSongID").drop("NewUserID");
		
		clickstream_prediction = clickstream_prediction.join(SongsMetaData,"SongID");
		clickstream_prediction = clickstream_prediction.drop("SongID");
		System.out.println("Done with Clickstream join with prediction data");
		//clickstream_prediction.show(300);
		//clickstream_prediction.coalesce(1).write().mode(SaveMode.Overwrite).format("csv").save("/Users/ravig/clickstream/");
		
		Dataset<Row> clickstream_prediction_count = clickstream_prediction.groupBy("prediction","ArtistID").count();
		//clickstream_prediction_count.show(100);
		Dataset<Row> Rank = clickstream_prediction_count.withColumn("rank", dense_rank().over(Window.partitionBy("prediction")
				.orderBy(desc("count"))));
		
		Rank=Rank.filter(Rank.col("rank").equalTo(1));
		System.out.println("Done with finding the dominant artist in a cluster");
		//Rank.show(300);
		//Rank.coalesce(1).write().mode(SaveMode.Overwrite).format("csv").save("/Users/ravig/clickstream/");
		Rank=Rank.withColumnRenamed("ArtistID", "TopArtist");
		Dataset<Row> cluster_top = clickstream_prediction.join(Rank,clickstream_prediction.col("prediction")
									.equalTo(Rank.col("prediction"))
									.and(clickstream_prediction.col("ArtistID").equalTo(Rank.col("TopArtist"))))
				.drop("count").drop("rank").drop("ListenedCount").drop(clickstream_prediction.col("prediction"))
				.drop("TopArtist");
		
		
		cluster_top.show();
		
/*Filtering Out the list of Users who have clicked on Notification received for the ID 9660 */
		
		Dataset <Row> Notification_user_9660 = Notification_Clicks.filter(Notification_Clicks.col("NotificationID").equalTo(9660));
		Dataset <Row> Artists_notify_9660 = Notification_details.filter(Notification_details.col("NotificationID").equalTo(9660));
		
		System.out.println("List of Users who have clicked for Notification 9660");
		Notification_user_9660.show();
		
		System.out.println("List of Artists for Notification 9660");
		Artists_notify_9660.show();
		
		Dataset <Row> cluster_users_9660_notified = Artists_notify_9660.join(cluster_top,"ArtistID");
		System.out.println("List of Users to whom we  sent Notification 9660");
		cluster_users_9660_notified.show();
		
		Dataset<Row> cluster_9660_clicks = cluster_users_9660_notified.join(Notification_user_9660,"UserID");
		System.out.println("List of Users to who clicked on Notification 9660");
		cluster_9660_clicks.show();
		
		Long Clicks = cluster_9660_clicks.count();
		Long Count = cluster_users_9660_notified.count();
		double CTR = 0.0;
		if (Clicks != 0)
		{
			CTR = Clicks/Count;	
		}
		else
		{
			System.out.println("CTR cannot be calculated since notificaiton was not sent to any users");
		}
		
		
		System.out.println("Click Stream ratio for 9660 is " +CTR*100);
	
				
				} 
}
